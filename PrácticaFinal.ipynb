{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PrácticaFinal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pimateos/Deep-Learning/blob/master/Pr%C3%A1cticaFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5H1nWhikvlVl"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Cuestionario (2.5 puntos)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ggztemaDwvQC"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 *(0.5 puntos)* Explicar por qué es preferible hacer np.dot(X, Y), donde X e Y son arrays bidimensionales, que ejecutar un triple bucle con el algoritmo básico de la multiplicación de matrices."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fmPuCTrqxFbS"
      },
      "cell_type": "markdown",
      "source": [
        "Porque numpy calcula en paralelo las matrices en lugar del metodo clasico que se itera."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FLtL4jdOxJNY"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 *(0.5 puntos)* Explicar qué ventajas nos aporta utilizar un grafo de computación a la hora de entrenar redes neuronales."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JvFvPIkix044"
      },
      "cell_type": "markdown",
      "source": [
        "Sabemos el orden que tienen las operaciones y si son compatibles.\n",
        "\n",
        "Mas velocidad de ejecución.\n",
        "\n",
        "Nos permite poder establecer el punto desde el cual queremos computar. Ademas de establecer paralelismos. Esto nos ayuda a ejecutar en entornos distribuidos y/o GPUs.\n",
        "\n",
        "Los frameworks modernos tienen operaciones embebidas en los nodos de los grafos, por ejemplo, el backward propagation.  Esto lo hacen automaticamente y es muy importante en redes muy grandes(autodiferenciación)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tsZHOJWdx2yV"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 *(0.5 puntos)* Hemos visto cómo con TensorFlow es posible añadir una operación de optimización (como *tf.train.GradientDescentOptimizer*) y aplicar con ésta la minimización de la función de pérdida. Explicar cómo afecta esto al grafo de computación y cuál es su relación con el entrenamiento de una red neuronal."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i_c_BBjzx25o"
      },
      "cell_type": "markdown",
      "source": [
        "Gradient Descent Optimizer nos permite calcular el array cuando tenemos variables variables minimizando el error de las mismas al valor que establezcamos, pero computandola optimizando el calculo de matrices."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "1vFhP_iYx281"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4 *(0.5 puntos)* ¿Qué diferencias hay entre grafos estáticos y grafos dinámicos?\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xcgjyogx0FOw"
      },
      "cell_type": "markdown",
      "source": [
        "Los grafos estáticos se calculan antes de ejecutar el código. Mientras que los grafos dinámicos se calculan mientras se estan ejecutando. Pytorch los calcula dinámicamente. Mientras que Tensorflow estáticamente en versiones antiguas. A partir de Tensorflow 2.0 se calculan por defecto dinámicamente y se puede cambiar a forma estática.\n",
        "\n",
        "Los grafos dinamicos son buenos para experimentación. Sin embargo, para producción son mejores los estáticos."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0Rm79mkb0FT3"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.5 *(0.5 puntos)* Algunas ops en redes neuronales tienen un comportamiento diferente durante *training time* (cuando entrenamos la red) que durante *inference time* (cuando utilizamos la red ya entrenada). Da un ejemplo de una operación usada en redes neuronales de este tipo y explicar la diferencia en ejecución. ¿Qué consecuencias tiene esto en relación al grafo de computación?"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "32uwO8Gc0FYf"
      },
      "cell_type": "markdown",
      "source": [
        "Dropout se utiliza para prevenir el overfitting. No teniendo en cuenta aleatoriamente una neurona. Consiguiendo con su introduccion en el entrenamiento, modelos mas robustos.\n",
        "\n",
        "En la parte de inferencia no tendria sentido utilizar esta técnica, ya que, ya tenemos entrenada la red y no tendria sentido meter un componente aleatorio en la predicción también. Porque la salida nos daria una predicción aleatoria.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mX8gZlVyCCbz"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Laboratorio (7.5 puntos)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "En este laboratorio, vamos a trabajar con Convolutional Neural Networks para resolver un problema de clasificación de imágenes. En particular, vamos a clasificar imágenes de personajes de la conocida serie de los Simpsons.\n",
        "\n",
        "Como las CNN profundas son un tipo de modelo bastante avanzado y computacionalmente costoso, se recomienda hacer la práctica en Google Colaboratory con soporte para GPUs. En [este enlace](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) se explica cómo activar un entorno con GPUs. *Nota: para leer las imágenes y estandarizarlas al mismo tamaño se usa la librería opencv. Esta ĺibrería está ya instalada en el entorno de Colab, pero si trabajáis de manera local tendréis que instalarla.*\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/i8zIGqX.jpg\" style=\"text-align: center\" height=\"300px\"></center>\n",
        "\n",
        "El dataset a utilizar consiste en imágenes de personajes de los Simpsons extraídas directamente de capítulos de la serie. Este dataset ha sido recopilado por [Alexandre Attia](http://www.alexattia.fr/) y es más complejo que el clásico dataset de MNIST. Aparte de tener más clases (vamos a utilizar los 18 personajes con más imágenes), los personajes pueden aparecer en distintas poses, en distintas posiciones de la imagen o con otros personajes en pantalla (si bien el personaje a clasificar siempre aparece en la posición predominante).\n",
        "\n",
        "El dataset de training puede ser descargado desde aquí:\n",
        "\n",
        "[Training data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60) (~500MB)\n",
        "\n",
        "Por otro lado, el dataset de test puede ser descargado de aquí:\n",
        "\n",
        "[Test data](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8) (~10MB)\n",
        "\n",
        "Antes de empezar la práctica, se recomienda descargar las imágenes y echarlas un vistazo.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QI274F8LQC59"
      },
      "cell_type": "markdown",
      "source": [
        "## Carga de los datos"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "D7tKOZ9BFfki",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np \n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "\n",
        "# Primero, bajamos los datos de entrenamiento\n",
        "keras.utils.get_file(fname=\"simpsons_train.tar.gz\", \n",
        "                     origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219337&authkey=AMzI92bJPx8Sd60\")\n",
        "\n",
        "# Descomprimimos el archivo\n",
        "!tar -xzf /root/.keras/datasets/simpsons_train.tar.gz -C /root/.keras/datasets\n",
        "\n",
        "# Hacemos lo mismo con los datos de test\n",
        "keras.utils.get_file(fname=\"simpsons_test.tar.gz\", \n",
        "                     origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219341&authkey=ANnjK3Uq1FhuAe8\")\n",
        "!tar -xzf /root/.keras/datasets/simpsons_test.tar.gz -C /root/.keras/datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hMFhe3COFwSD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Esta variable contiene un mapeo de número de clase a personaje.\n",
        "# Utilizamos sólo los 18 personajes del dataset que tienen más imágenes.\n",
        "MAP_CHARACTERS = {\n",
        "    0: 'abraham_grampa_simpson', 1: 'apu_nahasapeemapetilon', 2: 'bart_simpson',\n",
        "    3: 'charles_montgomery_burns', 4: 'chief_wiggum', 5: 'comic_book_guy', 6: 'edna_krabappel', \n",
        "    7: 'homer_simpson', 8: 'kent_brockman', 9: 'krusty_the_clown', 10: 'lisa_simpson', \n",
        "    11: 'marge_simpson', 12: 'milhouse_van_houten', 13: 'moe_szyslak', \n",
        "    14: 'ned_flanders', 15: 'nelson_muntz', 16: 'principal_skinner', 17: 'sideshow_bob'\n",
        "}\n",
        "\n",
        "# Vamos a standarizar todas las imágenes a tamaño 64x64\n",
        "IMG_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5bJ0NsbCbupF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_train_set(dirname, map_characters, verbose=True):\n",
        "    \"\"\"Esta función carga los datos de training en imágenes.\n",
        "    \n",
        "    Como las imágenes tienen tamaños distintas, utilizamos la librería opencv\n",
        "    para hacer un resize y adaptarlas todas a tamaño IMG_SIZE x IMG_SIZE.\n",
        "    \n",
        "    Args:\n",
        "        dirname: directorio completo del que leer los datos\n",
        "        map_characters: variable de mapeo entre labels y personajes\n",
        "        verbose: si es True, muestra información de las imágenes cargadas\n",
        "     \n",
        "    Returns:\n",
        "        X, y: X es un array con todas las imágenes cargadas con tamaño\n",
        "                IMG_SIZE x IMG_SIZE\n",
        "              y es un array con las labels de correspondientes a cada imagen\n",
        "    \"\"\"\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for label, character in map_characters.items():        \n",
        "        files = os.listdir(os.path.join(dirname, character))\n",
        "        images = [file for file in files if file.endswith(\"jpg\")]\n",
        "        if verbose:\n",
        "          print(\"Leyendo {} imágenes encontradas de {}\".format(len(images), character))\n",
        "        for image_name in images:\n",
        "            image = cv2.imread(os.path.join(dirname, character, image_name))\n",
        "            X_train.append(cv2.resize(image,(IMG_SIZE, IMG_SIZE)))\n",
        "            y_train.append(label)\n",
        "    return np.array(X_train), np.array(y_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NslxhnnDK6uA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_test_set(dirname, map_characters, verbose=True):\n",
        "    \"\"\"Esta función funciona de manera equivalente a la función load_train_set\n",
        "    pero cargando los datos de test.\"\"\"\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    reverse_dict = {v: k for k, v in map_characters.items()}\n",
        "    for filename in glob.glob(dirname + '/*.*'):\n",
        "        char_name = \"_\".join(filename.split('/')[-1].split('_')[:-1])\n",
        "        if char_name in reverse_dict:\n",
        "            image = cv2.imread(filename)\n",
        "            image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "            X_test.append(image)\n",
        "            y_test.append(reverse_dict[char_name])\n",
        "    if verbose:\n",
        "        print(\"Leídas {} imágenes de test\".format(len(X_test)))\n",
        "    return np.array(X_test), np.array(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WVWqKxFcbwTu",
        "outputId": "146da7e8-6505-46d2-ec38-018824dd86ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# Cargamos los datos. Si no estás trabajando en colab, cambia los paths por\n",
        "# los de los ficheros donde hayas descargado los datos.\n",
        "DATASET_TRAIN_PATH_COLAB = \"/root/.keras/datasets/simpsons\"\n",
        "DATASET_TEST_PATH_COLAB = \"/root/.keras/datasets/simpsons_testset\"\n",
        "\n",
        "X, y = load_train_set(DATASET_TRAIN_PATH_COLAB, MAP_CHARACTERS)\n",
        "X_t, y_t = load_test_set(DATASET_TEST_PATH_COLAB, MAP_CHARACTERS)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Leyendo 913 imágenes encontradas de abraham_grampa_simpson\n",
            "Leyendo 623 imágenes encontradas de apu_nahasapeemapetilon\n",
            "Leyendo 1342 imágenes encontradas de bart_simpson\n",
            "Leyendo 1193 imágenes encontradas de charles_montgomery_burns\n",
            "Leyendo 986 imágenes encontradas de chief_wiggum\n",
            "Leyendo 469 imágenes encontradas de comic_book_guy\n",
            "Leyendo 457 imágenes encontradas de edna_krabappel\n",
            "Leyendo 2246 imágenes encontradas de homer_simpson\n",
            "Leyendo 498 imágenes encontradas de kent_brockman\n",
            "Leyendo 1206 imágenes encontradas de krusty_the_clown\n",
            "Leyendo 1354 imágenes encontradas de lisa_simpson\n",
            "Leyendo 1291 imágenes encontradas de marge_simpson\n",
            "Leyendo 1079 imágenes encontradas de milhouse_van_houten\n",
            "Leyendo 1452 imágenes encontradas de moe_szyslak\n",
            "Leyendo 1454 imágenes encontradas de ned_flanders\n",
            "Leyendo 358 imágenes encontradas de nelson_muntz\n",
            "Leyendo 1194 imágenes encontradas de principal_skinner\n",
            "Leyendo 877 imágenes encontradas de sideshow_bob\n",
            "Leídas 890 imágenes de test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pBbmz9DMhVhc"
      },
      "cell_type": "markdown",
      "source": [
        "## Entregable\n",
        "\n",
        "Utilizando Convolutional Neural Networks con TensorFlow (ya sea tf.keras, TensorFlow Eager, o TensorFlow tradicional), entrenar un clasificador que sea capaz de reconocer personajes en imágenes de los Simpsons con una accuracy en el dataset de test de **88%**. Redactar un informe analizando varias de las alternativas probadas y los resultados obtenidos.\n",
        "\n",
        "El entregable (en formato .ipynb, aunque puede apoyarse con un archivo pdf) tiene que contener los siguientes elementos:\n",
        "\n",
        "**1 (1.5 puntos): Implementación con una red neuronal clásica**. Implementar primero un modelo feed-forward clásico, sin convoluciones. Mostrar código y  resultados de entrenamiento y accuracy en el test set.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zu2H1KqzsNdu",
        "colab_type": "code",
        "outputId": "2950a701-00f5-4a9f-a0cb-465f3d1c9149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(X_t.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18992, 64, 64, 3)\n",
            "(890, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n_4L-HYngDje",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    \n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iwq-DPLbqzG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7m9YDLS7Ef9r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_classes = 18\n",
        "epochs = 20\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2HL3OIeUCWQx",
        "colab_type": "code",
        "outputId": "1454e274-9a54-4fd2-c37b-5fa3ac1d5f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# the data, split between train and test sets\n",
        "img_rows=64\n",
        "img_cols=64\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = X_t.reshape(X_t.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = X.reshape(X.shape[0], img_rows, img_cols,3)\n",
        "    x_test = X_t.reshape(X_t.shape[0], img_rows, img_cols,3)\n",
        "    input_shape = (img_rows, img_cols,3)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train= y\n",
        "y_test=y_t\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (18992, 64, 64, 3)\n",
            "18992 train samples\n",
            "890 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SmJW2gmtuSOC",
        "colab_type": "code",
        "outputId": "bd66b1c8-9091-4370-918f-2d0b4a234c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(64*64*3)\n",
        "print(input_shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12288\n",
            "(64, 64, 3)\n",
            "(890, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DsxJ4lINtw4I",
        "colab_type": "code",
        "outputId": "a0c475fa-5089-4b70-ee71-9dbbae5c33f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(3, activation='relu', input_shape=input_shape))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 64, 64, 3)         12        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64, 64, 512)       2048      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64, 64, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2097152)           0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 18)                37748754  \n",
            "=================================================================\n",
            "Total params: 37,750,814\n",
            "Trainable params: 37,750,814\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jZAcCmK5t0IK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),shuffle=True)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B4GD0ck6q0SE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2 (4.5 puntos): Implementación con CNNs**. Implementar una arquitectura CNN que consiga un 88% de accuracy en el test set. Mostrar código y resultados de entrenamiento y accuracy en el test set. Explicar brevemente la arquitectura utilizada y las pruebas realizadas.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZrSD_W_WcQbX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementacion con CNN"
      ]
    },
    {
      "metadata": {
        "id": "h2uOAt1Vu_kd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from timeit import default_timer as timer\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "image_height = 64\n",
        "image_width = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FETKEBIS8aI7",
        "colab_type": "code",
        "outputId": "b3d1fe95-8087-4a70-bc15-ff5a7e0a9ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# Cargamos los datos. Si no estás trabajando en colab, cambia los paths por\n",
        "# los de los ficheros donde hayas descargado los datos.\n",
        "DATASET_TRAIN_PATH_COLAB = \"/root/.keras/datasets/simpsons\"\n",
        "DATASET_TEST_PATH_COLAB = \"/root/.keras/datasets/simpsons_testset\"\n",
        "\n",
        "X, y = load_train_set(DATASET_TRAIN_PATH_COLAB, MAP_CHARACTERS)\n",
        "X_t, y_t = load_test_set(DATASET_TEST_PATH_COLAB, MAP_CHARACTERS)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Leyendo 913 imágenes encontradas de abraham_grampa_simpson\n",
            "Leyendo 623 imágenes encontradas de apu_nahasapeemapetilon\n",
            "Leyendo 1342 imágenes encontradas de bart_simpson\n",
            "Leyendo 1193 imágenes encontradas de charles_montgomery_burns\n",
            "Leyendo 986 imágenes encontradas de chief_wiggum\n",
            "Leyendo 469 imágenes encontradas de comic_book_guy\n",
            "Leyendo 457 imágenes encontradas de edna_krabappel\n",
            "Leyendo 2246 imágenes encontradas de homer_simpson\n",
            "Leyendo 498 imágenes encontradas de kent_brockman\n",
            "Leyendo 1206 imágenes encontradas de krusty_the_clown\n",
            "Leyendo 1354 imágenes encontradas de lisa_simpson\n",
            "Leyendo 1291 imágenes encontradas de marge_simpson\n",
            "Leyendo 1079 imágenes encontradas de milhouse_van_houten\n",
            "Leyendo 1452 imágenes encontradas de moe_szyslak\n",
            "Leyendo 1454 imágenes encontradas de ned_flanders\n",
            "Leyendo 358 imágenes encontradas de nelson_muntz\n",
            "Leyendo 1194 imágenes encontradas de principal_skinner\n",
            "Leyendo 877 imágenes encontradas de sideshow_bob\n",
            "Leídas 890 imágenes de test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zjmOy024A6SB",
        "colab_type": "code",
        "outputId": "7bdc472d-2086-45b8-8e64-3204c9ad9598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 18\n",
        "epochs = 20\n",
        "#Se realiza el reshape y se normaliza para poder entrenar la red neuronal\n",
        "img_rows=64\n",
        "img_cols=64\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = X.reshape(X.shape[0], 3, img_rows, img_cols)\n",
        "    x_test = X_t.reshape(X_t.shape[0],3, img_rows, img_cols)\n",
        "    input_shape = (3, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = X.reshape(X.shape[0], img_rows, img_cols,3)\n",
        "    x_test = X_t.reshape(X_t.shape[0], img_rows, img_cols,3)\n",
        "    input_shape = (img_rows, img_cols,3)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train= y\n",
        "y_test=y_t\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Se convierte las clases en matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (18992, 64, 64, 3)\n",
            "18992 train samples\n",
            "890 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kishhiUPEZdc",
        "colab_type": "code",
        "outputId": "81e89717-233f-4000-d755-59c69f47f434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18992, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "mTjxYRo3EeTU",
        "colab_type": "code",
        "outputId": "71ccbc89-43d9-4a4b-aefe-e3f0494583b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(890, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "bjmNyihvCPsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "img_width, img_height = 64, 64\n",
        "#Se establece el input_shape de la red neuronal con los channels y las dimensiones de las imagenes 64 y 64 (altura y anchura)\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (img_width, img_height,3)\n",
        "else:\n",
        "    input_shape = (3,img_width, img_height)\n",
        "#Se crea la red neuronal convolucional con 3 capas convolucionales y 3 capas densas. Se añade dropout del 50% para que no se sobreentrene y como es tenemos 18 clases\n",
        "#se añade una capa final densa con el numero de clases 18 y con funcion de activacion softmax\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(64,64,3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(18))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01uVCuF4XAek",
        "colab_type": "code",
        "outputId": "fb467e4e-c5f9-4455-daee-81af1e2c086d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4206
        }
      },
      "cell_type": "code",
      "source": [
        "#Se entrena\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),shuffle=True)\n",
        "#Se calcula como se comporta\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "#Se muestra graficamente\n",
        "plot_history(history)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18992 samples, validate on 890 samples\n",
            "Epoch 1/100\n",
            "18992/18992 [==============================] - 20s 1ms/step - loss: 2.7046 - acc: 0.1476 - val_loss: 2.5055 - val_acc: 0.2011\n",
            "Epoch 2/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 2.2276 - acc: 0.3039 - val_loss: 1.9965 - val_acc: 0.4427\n",
            "Epoch 3/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 1.8307 - acc: 0.4321 - val_loss: 1.4656 - val_acc: 0.5674\n",
            "Epoch 4/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 1.5457 - acc: 0.5168 - val_loss: 1.1508 - val_acc: 0.6449\n",
            "Epoch 5/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 1.3288 - acc: 0.5815 - val_loss: 1.0169 - val_acc: 0.7022\n",
            "Epoch 6/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 1.1569 - acc: 0.6314 - val_loss: 0.8301 - val_acc: 0.7472\n",
            "Epoch 7/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 1.0343 - acc: 0.6667 - val_loss: 0.7193 - val_acc: 0.7775\n",
            "Epoch 8/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.9117 - acc: 0.7079 - val_loss: 0.5927 - val_acc: 0.8270\n",
            "Epoch 9/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.8354 - acc: 0.7265 - val_loss: 0.5109 - val_acc: 0.8382\n",
            "Epoch 10/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.7504 - acc: 0.7516 - val_loss: 0.4472 - val_acc: 0.8663\n",
            "Epoch 11/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.6840 - acc: 0.7697 - val_loss: 0.3879 - val_acc: 0.8888\n",
            "Epoch 12/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.6480 - acc: 0.7810 - val_loss: 0.3491 - val_acc: 0.9101\n",
            "Epoch 13/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.6046 - acc: 0.7979 - val_loss: 0.3015 - val_acc: 0.9191\n",
            "Epoch 14/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.5705 - acc: 0.8064 - val_loss: 0.2904 - val_acc: 0.9191\n",
            "Epoch 15/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.5315 - acc: 0.8205 - val_loss: 0.2393 - val_acc: 0.9483\n",
            "Epoch 16/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.4969 - acc: 0.8325 - val_loss: 0.2104 - val_acc: 0.9573\n",
            "Epoch 17/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.4773 - acc: 0.8402 - val_loss: 0.1968 - val_acc: 0.9506\n",
            "Epoch 18/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.4599 - acc: 0.8458 - val_loss: 0.2020 - val_acc: 0.9427\n",
            "Epoch 19/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.4388 - acc: 0.8510 - val_loss: 0.1609 - val_acc: 0.9596\n",
            "Epoch 20/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.4342 - acc: 0.8569 - val_loss: 0.1570 - val_acc: 0.9685\n",
            "Epoch 21/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.4016 - acc: 0.8647 - val_loss: 0.1377 - val_acc: 0.9708\n",
            "Epoch 22/100\n",
            "18992/18992 [==============================] - 16s 839us/step - loss: 0.3979 - acc: 0.8665 - val_loss: 0.1457 - val_acc: 0.9618\n",
            "Epoch 23/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.3796 - acc: 0.8746 - val_loss: 0.1229 - val_acc: 0.9719\n",
            "Epoch 24/100\n",
            "18992/18992 [==============================] - 16s 833us/step - loss: 0.3730 - acc: 0.8763 - val_loss: 0.1269 - val_acc: 0.9708\n",
            "Epoch 25/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.3595 - acc: 0.8813 - val_loss: 0.0992 - val_acc: 0.9742\n",
            "Epoch 26/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.3567 - acc: 0.8834 - val_loss: 0.1153 - val_acc: 0.9798\n",
            "Epoch 27/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.3352 - acc: 0.8881 - val_loss: 0.0979 - val_acc: 0.9809\n",
            "Epoch 28/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.3359 - acc: 0.8887 - val_loss: 0.1019 - val_acc: 0.9719\n",
            "Epoch 29/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.3221 - acc: 0.8943 - val_loss: 0.0857 - val_acc: 0.9831\n",
            "Epoch 30/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.3049 - acc: 0.8988 - val_loss: 0.0921 - val_acc: 0.9843\n",
            "Epoch 31/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.3065 - acc: 0.9020 - val_loss: 0.0930 - val_acc: 0.9775\n",
            "Epoch 32/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.2981 - acc: 0.8994 - val_loss: 0.0743 - val_acc: 0.9865\n",
            "Epoch 33/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.3085 - acc: 0.9009 - val_loss: 0.0733 - val_acc: 0.9854\n",
            "Epoch 34/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.2861 - acc: 0.9069 - val_loss: 0.0656 - val_acc: 0.9888\n",
            "Epoch 35/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.2721 - acc: 0.9114 - val_loss: 0.0626 - val_acc: 0.9888\n",
            "Epoch 36/100\n",
            "18992/18992 [==============================] - 16s 830us/step - loss: 0.2711 - acc: 0.9126 - val_loss: 0.0669 - val_acc: 0.9876\n",
            "Epoch 37/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.2544 - acc: 0.9156 - val_loss: 0.0770 - val_acc: 0.9843\n",
            "Epoch 38/100\n",
            "18992/18992 [==============================] - 16s 829us/step - loss: 0.2513 - acc: 0.9184 - val_loss: 0.0860 - val_acc: 0.9809\n",
            "Epoch 39/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.2561 - acc: 0.9155 - val_loss: 0.0759 - val_acc: 0.9854\n",
            "Epoch 40/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.2676 - acc: 0.9135 - val_loss: 0.0478 - val_acc: 0.9899\n",
            "Epoch 41/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.2724 - acc: 0.9120 - val_loss: 0.0427 - val_acc: 0.9921\n",
            "Epoch 42/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.2476 - acc: 0.9207 - val_loss: 0.0457 - val_acc: 0.9933\n",
            "Epoch 43/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.2454 - acc: 0.9203 - val_loss: 0.0454 - val_acc: 0.9955\n",
            "Epoch 44/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.2473 - acc: 0.9198 - val_loss: 0.0403 - val_acc: 0.9933\n",
            "Epoch 45/100\n",
            "18992/18992 [==============================] - 16s 840us/step - loss: 0.2415 - acc: 0.9222 - val_loss: 0.0475 - val_acc: 0.9944\n",
            "Epoch 46/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.2436 - acc: 0.9211 - val_loss: 0.0413 - val_acc: 0.9944\n",
            "Epoch 47/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.2315 - acc: 0.9248 - val_loss: 0.0482 - val_acc: 0.9910\n",
            "Epoch 48/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.2296 - acc: 0.9254 - val_loss: 0.0392 - val_acc: 0.9978\n",
            "Epoch 49/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.2275 - acc: 0.9267 - val_loss: 0.0491 - val_acc: 0.9910\n",
            "Epoch 50/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.2133 - acc: 0.9322 - val_loss: 0.0319 - val_acc: 0.9955\n",
            "Epoch 51/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.2229 - acc: 0.9288 - val_loss: 0.0397 - val_acc: 0.9944\n",
            "Epoch 52/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.2240 - acc: 0.9304 - val_loss: 0.0401 - val_acc: 0.9966\n",
            "Epoch 53/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.2068 - acc: 0.9339 - val_loss: 0.0329 - val_acc: 0.9966\n",
            "Epoch 54/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.2115 - acc: 0.9330 - val_loss: 0.0453 - val_acc: 0.9933\n",
            "Epoch 55/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1970 - acc: 0.9349 - val_loss: 0.0264 - val_acc: 0.9955\n",
            "Epoch 56/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.2183 - acc: 0.9309 - val_loss: 0.0304 - val_acc: 0.9966\n",
            "Epoch 57/100\n",
            "18992/18992 [==============================] - 16s 839us/step - loss: 0.1991 - acc: 0.9363 - val_loss: 0.0283 - val_acc: 0.9989\n",
            "Epoch 58/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.2017 - acc: 0.9360 - val_loss: 0.0313 - val_acc: 0.9955\n",
            "Epoch 59/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1892 - acc: 0.9390 - val_loss: 0.0229 - val_acc: 0.9989\n",
            "Epoch 60/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1993 - acc: 0.9372 - val_loss: 0.0308 - val_acc: 0.9955\n",
            "Epoch 61/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.2006 - acc: 0.9359 - val_loss: 0.0381 - val_acc: 0.9944\n",
            "Epoch 62/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1980 - acc: 0.9384 - val_loss: 0.0323 - val_acc: 0.9966\n",
            "Epoch 63/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.2020 - acc: 0.9356 - val_loss: 0.0356 - val_acc: 0.9933\n",
            "Epoch 64/100\n",
            "18992/18992 [==============================] - 16s 834us/step - loss: 0.1930 - acc: 0.9386 - val_loss: 0.0405 - val_acc: 0.9944\n",
            "Epoch 65/100\n",
            "18992/18992 [==============================] - 16s 829us/step - loss: 0.1879 - acc: 0.9401 - val_loss: 0.0304 - val_acc: 0.9966\n",
            "Epoch 66/100\n",
            "18992/18992 [==============================] - 16s 829us/step - loss: 0.1909 - acc: 0.9426 - val_loss: 0.0311 - val_acc: 0.9955\n",
            "Epoch 67/100\n",
            "18992/18992 [==============================] - 16s 832us/step - loss: 0.1862 - acc: 0.9419 - val_loss: 0.0263 - val_acc: 0.9978\n",
            "Epoch 68/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.1820 - acc: 0.9431 - val_loss: 0.0243 - val_acc: 0.9978\n",
            "Epoch 69/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1845 - acc: 0.9438 - val_loss: 0.0280 - val_acc: 0.9966\n",
            "Epoch 70/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1793 - acc: 0.9440 - val_loss: 0.0295 - val_acc: 0.9966\n",
            "Epoch 71/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1808 - acc: 0.9433 - val_loss: 0.0225 - val_acc: 0.9989\n",
            "Epoch 72/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.1685 - acc: 0.9458 - val_loss: 0.0223 - val_acc: 0.9989\n",
            "Epoch 73/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1734 - acc: 0.9445 - val_loss: 0.0395 - val_acc: 0.9921\n",
            "Epoch 74/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1743 - acc: 0.9448 - val_loss: 0.0428 - val_acc: 0.9955\n",
            "Epoch 75/100\n",
            "18992/18992 [==============================] - 16s 838us/step - loss: 0.1795 - acc: 0.9424 - val_loss: 0.0232 - val_acc: 0.9978\n",
            "Epoch 76/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1532 - acc: 0.9520 - val_loss: 0.0163 - val_acc: 0.9989\n",
            "Epoch 77/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.1696 - acc: 0.9462 - val_loss: 0.0224 - val_acc: 0.9978\n",
            "Epoch 78/100\n",
            "18992/18992 [==============================] - 16s 832us/step - loss: 0.1652 - acc: 0.9480 - val_loss: 0.0288 - val_acc: 0.9955\n",
            "Epoch 79/100\n",
            "18992/18992 [==============================] - 16s 829us/step - loss: 0.1596 - acc: 0.9492 - val_loss: 0.0241 - val_acc: 0.9978\n",
            "Epoch 80/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.1665 - acc: 0.9486 - val_loss: 0.0187 - val_acc: 0.9989\n",
            "Epoch 81/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1540 - acc: 0.9495 - val_loss: 0.0246 - val_acc: 0.9966\n",
            "Epoch 82/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1659 - acc: 0.9494 - val_loss: 0.0207 - val_acc: 0.9978\n",
            "Epoch 83/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.1540 - acc: 0.9523 - val_loss: 0.0176 - val_acc: 0.9978\n",
            "Epoch 84/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.1501 - acc: 0.9531 - val_loss: 0.0197 - val_acc: 0.9978\n",
            "Epoch 85/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.1538 - acc: 0.9521 - val_loss: 0.0195 - val_acc: 0.9989\n",
            "Epoch 86/100\n",
            "18992/18992 [==============================] - 16s 837us/step - loss: 0.1575 - acc: 0.9509 - val_loss: 0.0303 - val_acc: 0.9933\n",
            "Epoch 87/100\n",
            "18992/18992 [==============================] - 16s 836us/step - loss: 0.1646 - acc: 0.9486 - val_loss: 0.0222 - val_acc: 0.9955\n",
            "Epoch 88/100\n",
            "18992/18992 [==============================] - 16s 829us/step - loss: 0.1504 - acc: 0.9536 - val_loss: 0.0337 - val_acc: 0.9966\n",
            "Epoch 89/100\n",
            "18992/18992 [==============================] - 16s 833us/step - loss: 0.1520 - acc: 0.9531 - val_loss: 0.0195 - val_acc: 0.9989\n",
            "Epoch 90/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1479 - acc: 0.9555 - val_loss: 0.0229 - val_acc: 0.9978\n",
            "Epoch 91/100\n",
            "18992/18992 [==============================] - 16s 835us/step - loss: 0.1510 - acc: 0.9531 - val_loss: 0.0184 - val_acc: 0.9978\n",
            "Epoch 92/100\n",
            "18992/18992 [==============================] - 16s 830us/step - loss: 0.1434 - acc: 0.9549 - val_loss: 0.0220 - val_acc: 0.9966\n",
            "Epoch 93/100\n",
            "18992/18992 [==============================] - 16s 829us/step - loss: 0.1416 - acc: 0.9565 - val_loss: 0.0158 - val_acc: 0.9989\n",
            "Epoch 94/100\n",
            "18992/18992 [==============================] - 16s 830us/step - loss: 0.1418 - acc: 0.9560 - val_loss: 0.0276 - val_acc: 0.9989\n",
            "Epoch 95/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.1484 - acc: 0.9536 - val_loss: 0.0185 - val_acc: 0.9966\n",
            "Epoch 96/100\n",
            "18992/18992 [==============================] - 16s 830us/step - loss: 0.1407 - acc: 0.9550 - val_loss: 0.0143 - val_acc: 0.9978\n",
            "Epoch 97/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.1324 - acc: 0.9581 - val_loss: 0.0250 - val_acc: 0.9989\n",
            "Epoch 98/100\n",
            "18992/18992 [==============================] - 16s 831us/step - loss: 0.1333 - acc: 0.9584 - val_loss: 0.0126 - val_acc: 0.9989\n",
            "Epoch 99/100\n",
            "18992/18992 [==============================] - 16s 830us/step - loss: 0.1436 - acc: 0.9551 - val_loss: 0.0147 - val_acc: 0.9989\n",
            "Epoch 100/100\n",
            "18992/18992 [==============================] - 16s 832us/step - loss: 0.1434 - acc: 0.9556 - val_loss: 0.0184 - val_acc: 0.9989\n",
            "Test loss: 0.018386583848531996\n",
            "Test accuracy: 0.998876404494382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMAyhDcAAJYhvAEAsAzhDQCAZf4XMuU9fR6Pt6EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_SUuZziCvDSP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Probamos otra arquitectura de red con 4 capas convolucionales y distintos numeros de neuronas aumentando de 64 a 128 a 256 y finalmente disminuyendo y compleentando con 3 capas densas. La final con 18 neuronas, numero de clases a clasificar."
      ]
    },
    {
      "metadata": {
        "id": "hEiwA2LJdIPv",
        "colab_type": "code",
        "outputId": "4e9ca167-c40f-46e6-80d4-7ff7d89334a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2842
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "img_width, img_height = 64, 64\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = ( 3,img_width, img_height)\n",
        "else:\n",
        "    input_shape = (3,img_width, img_height)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), input_shape=(64,64,3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(18))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test),shuffle=True)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18992 samples, validate on 890 samples\n",
            "Epoch 1/100\n",
            "18992/18992 [==============================] - 78s 4ms/step - loss: 2.6879 - acc: 0.1525 - val_loss: 2.5695 - val_acc: 0.1798\n",
            "Epoch 2/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 2.3619 - acc: 0.2592 - val_loss: 2.2548 - val_acc: 0.3056\n",
            "Epoch 3/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 2.1007 - acc: 0.3416 - val_loss: 1.9698 - val_acc: 0.3966\n",
            "Epoch 4/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 1.8610 - acc: 0.4205 - val_loss: 1.6699 - val_acc: 0.4742\n",
            "Epoch 5/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 1.6644 - acc: 0.4812 - val_loss: 1.4651 - val_acc: 0.5360\n",
            "Epoch 6/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 1.4888 - acc: 0.5312 - val_loss: 1.2606 - val_acc: 0.5989\n",
            "Epoch 7/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 1.3148 - acc: 0.5852 - val_loss: 1.0080 - val_acc: 0.6831\n",
            "Epoch 8/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 1.1575 - acc: 0.6341 - val_loss: 0.8611 - val_acc: 0.7258\n",
            "Epoch 9/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 1.0072 - acc: 0.6813 - val_loss: 0.7304 - val_acc: 0.7876\n",
            "Epoch 10/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.8958 - acc: 0.7174 - val_loss: 0.5698 - val_acc: 0.8315\n",
            "Epoch 11/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.7799 - acc: 0.7549 - val_loss: 0.4781 - val_acc: 0.8640\n",
            "Epoch 12/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.6883 - acc: 0.7769 - val_loss: 0.4021 - val_acc: 0.8955\n",
            "Epoch 13/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.6264 - acc: 0.8013 - val_loss: 0.3716 - val_acc: 0.9169\n",
            "Epoch 14/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.5867 - acc: 0.8188 - val_loss: 0.3010 - val_acc: 0.9303\n",
            "Epoch 15/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.5219 - acc: 0.8350 - val_loss: 0.2508 - val_acc: 0.9404\n",
            "Epoch 16/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.4755 - acc: 0.8456 - val_loss: 0.2220 - val_acc: 0.9506\n",
            "Epoch 17/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.4496 - acc: 0.8609 - val_loss: 0.2041 - val_acc: 0.9551\n",
            "Epoch 18/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.4096 - acc: 0.8701 - val_loss: 0.1641 - val_acc: 0.9596\n",
            "Epoch 19/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.3858 - acc: 0.8805 - val_loss: 0.1402 - val_acc: 0.9652\n",
            "Epoch 20/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.3585 - acc: 0.8875 - val_loss: 0.1350 - val_acc: 0.9730\n",
            "Epoch 21/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.3495 - acc: 0.8928 - val_loss: 0.1230 - val_acc: 0.9753\n",
            "Epoch 22/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.3287 - acc: 0.9003 - val_loss: 0.1053 - val_acc: 0.9820\n",
            "Epoch 23/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.3331 - acc: 0.8961 - val_loss: 0.1244 - val_acc: 0.9685\n",
            "Epoch 24/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.3230 - acc: 0.8973 - val_loss: 0.1022 - val_acc: 0.9798\n",
            "Epoch 25/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2950 - acc: 0.9070 - val_loss: 0.0888 - val_acc: 0.9798\n",
            "Epoch 26/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2783 - acc: 0.9145 - val_loss: 0.0818 - val_acc: 0.9798\n",
            "Epoch 27/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2571 - acc: 0.9186 - val_loss: 0.0657 - val_acc: 0.9843\n",
            "Epoch 28/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2609 - acc: 0.9181 - val_loss: 0.0935 - val_acc: 0.9775\n",
            "Epoch 29/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2547 - acc: 0.9218 - val_loss: 0.0639 - val_acc: 0.9865\n",
            "Epoch 30/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2405 - acc: 0.9245 - val_loss: 0.0618 - val_acc: 0.9865\n",
            "Epoch 31/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2292 - acc: 0.9272 - val_loss: 0.0692 - val_acc: 0.9876\n",
            "Epoch 32/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2345 - acc: 0.9293 - val_loss: 0.0545 - val_acc: 0.9921\n",
            "Epoch 33/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2256 - acc: 0.9300 - val_loss: 0.0507 - val_acc: 0.9910\n",
            "Epoch 34/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2266 - acc: 0.9317 - val_loss: 0.0462 - val_acc: 0.9876\n",
            "Epoch 35/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2087 - acc: 0.9342 - val_loss: 0.0517 - val_acc: 0.9888\n",
            "Epoch 36/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2146 - acc: 0.9352 - val_loss: 0.0487 - val_acc: 0.9933\n",
            "Epoch 37/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1982 - acc: 0.9396 - val_loss: 0.0521 - val_acc: 0.9899\n",
            "Epoch 38/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2016 - acc: 0.9380 - val_loss: 0.0590 - val_acc: 0.9876\n",
            "Epoch 39/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1967 - acc: 0.9387 - val_loss: 0.0400 - val_acc: 0.9933\n",
            "Epoch 40/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.2006 - acc: 0.9376 - val_loss: 0.0379 - val_acc: 0.9944\n",
            "Epoch 41/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1859 - acc: 0.9444 - val_loss: 0.0408 - val_acc: 0.9944\n",
            "Epoch 42/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1851 - acc: 0.9433 - val_loss: 0.0357 - val_acc: 0.9978\n",
            "Epoch 43/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1723 - acc: 0.9470 - val_loss: 0.0362 - val_acc: 0.9955\n",
            "Epoch 44/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1797 - acc: 0.9439 - val_loss: 0.0334 - val_acc: 0.9966\n",
            "Epoch 45/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1745 - acc: 0.9463 - val_loss: 0.0306 - val_acc: 0.9966\n",
            "Epoch 46/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1737 - acc: 0.9462 - val_loss: 0.0333 - val_acc: 0.9966\n",
            "Epoch 47/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1575 - acc: 0.9521 - val_loss: 0.0292 - val_acc: 0.9978\n",
            "Epoch 48/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1714 - acc: 0.9478 - val_loss: 0.0331 - val_acc: 0.9955\n",
            "Epoch 49/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1634 - acc: 0.9498 - val_loss: 0.0384 - val_acc: 0.9978\n",
            "Epoch 50/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1733 - acc: 0.9477 - val_loss: 0.0293 - val_acc: 0.9944\n",
            "Epoch 51/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1542 - acc: 0.9530 - val_loss: 0.0296 - val_acc: 0.9966\n",
            "Epoch 52/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1467 - acc: 0.9553 - val_loss: 0.0262 - val_acc: 0.9978\n",
            "Epoch 53/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1482 - acc: 0.9559 - val_loss: 0.0264 - val_acc: 0.9978\n",
            "Epoch 54/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1554 - acc: 0.9542 - val_loss: 0.0340 - val_acc: 0.9966\n",
            "Epoch 55/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1494 - acc: 0.9552 - val_loss: 0.0295 - val_acc: 0.9944\n",
            "Epoch 56/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1560 - acc: 0.9530 - val_loss: 0.0303 - val_acc: 0.9966\n",
            "Epoch 57/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1361 - acc: 0.9567 - val_loss: 0.0271 - val_acc: 0.9944\n",
            "Epoch 58/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1360 - acc: 0.9592 - val_loss: 0.0304 - val_acc: 0.9966\n",
            "Epoch 59/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1470 - acc: 0.9553 - val_loss: 0.0382 - val_acc: 0.9944\n",
            "Epoch 60/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1494 - acc: 0.9539 - val_loss: 0.0268 - val_acc: 0.9933\n",
            "Epoch 61/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1369 - acc: 0.9592 - val_loss: 0.0330 - val_acc: 0.9933\n",
            "Epoch 62/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1367 - acc: 0.9584 - val_loss: 0.0258 - val_acc: 0.9944\n",
            "Epoch 63/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1269 - acc: 0.9606 - val_loss: 0.0235 - val_acc: 0.9978\n",
            "Epoch 64/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1352 - acc: 0.9588 - val_loss: 0.0204 - val_acc: 0.9978\n",
            "Epoch 65/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1266 - acc: 0.9595 - val_loss: 0.0257 - val_acc: 0.9989\n",
            "Epoch 66/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1339 - acc: 0.9615 - val_loss: 0.0598 - val_acc: 0.9933\n",
            "Epoch 67/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1370 - acc: 0.9595 - val_loss: 0.0236 - val_acc: 0.9966\n",
            "Epoch 68/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1239 - acc: 0.9621 - val_loss: 0.0266 - val_acc: 0.9944\n",
            "Epoch 69/100\n",
            "18992/18992 [==============================] - 69s 4ms/step - loss: 0.1177 - acc: 0.9623 - val_loss: 0.0261 - val_acc: 0.9978\n",
            "Epoch 70/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1147 - acc: 0.9648 - val_loss: 0.0284 - val_acc: 0.9978\n",
            "Epoch 71/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1137 - acc: 0.9646 - val_loss: 0.0237 - val_acc: 0.9966\n",
            "Epoch 72/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1193 - acc: 0.9645 - val_loss: 0.0240 - val_acc: 0.9978\n",
            "Epoch 73/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1216 - acc: 0.9629 - val_loss: 0.0251 - val_acc: 0.9955\n",
            "Epoch 74/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1127 - acc: 0.9655 - val_loss: 0.0252 - val_acc: 0.9966\n",
            "Epoch 75/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1167 - acc: 0.9649 - val_loss: 0.0281 - val_acc: 0.9955\n",
            "Epoch 76/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1204 - acc: 0.9635 - val_loss: 0.0315 - val_acc: 0.9966\n",
            "Epoch 77/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1226 - acc: 0.9635 - val_loss: 0.0267 - val_acc: 0.9921\n",
            "Epoch 78/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1070 - acc: 0.9675 - val_loss: 0.0294 - val_acc: 0.9933\n",
            "Epoch 79/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1087 - acc: 0.9675 - val_loss: 0.0221 - val_acc: 0.9978\n",
            "Epoch 80/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1247 - acc: 0.9625 - val_loss: 0.0341 - val_acc: 0.9944\n",
            "Epoch 81/100\n",
            "18992/18992 [==============================] - 68s 4ms/step - loss: 0.1101 - acc: 0.9666 - val_loss: 0.0315 - val_acc: 0.9944\n",
            "Epoch 82/100\n",
            "15872/18992 [========================>.....] - ETA: 11s - loss: 0.1524 - acc: 0.9560"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4X_vvYQysv3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3 (1 punto): Comparación GPU/CPU**. Comparar el tiempo de entrenamiento de tu modelo por epoch utilizando GPUs y CPUs. Explicar por qué el uso de una aceleradora como una GPU acelera el entrenamiento de la red neuronal."
      ]
    },
    {
      "metadata": {
        "id": "el5xzijCsxC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  start_gpu = time.time()\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(x_train, y_train, batch_size=10, nb_epoch=10, verbose=1, validation_data=(x_test, y_test))\n",
        "  loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "  print (\"Compilation Time : \", time.time() - start_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BYzrpe_pxVh3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  start_cpu = time.time()\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.fit(x_train, y_train, batch_size=10, nb_epoch=10, verbose=1, validation_data=(x_test, y_test))\n",
        "  loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "  print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "  print (\"Compilation Time : \", time.time() - start_cpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WFjeP9WXsybK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "sJtDm1vyrAha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*texto en cursiva*"
      ]
    },
    {
      "metadata": {
        "id": "qoMAXX9VtFom",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4 (0.5 puntos): Utilización de tf.data**. Si no se ha entrenado el modelo en el paso 2 con tf.data, repetir la ingesta de datos (incluyendo el preprocesamiento de imágenes) para el modelo utilizando tf.data."
      ]
    },
    {
      "metadata": {
        "id": "nhmkZL1vymT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create the training datasets\n",
        "dx_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "# apply a one-hot transformation to each label for use in the neural network\n",
        "dy_train = tf.data.Dataset.from_tensor_slices(y_train).map(lambda z: tf.one_hot(z, 10))\n",
        "# zip the x and y training data together and shuffle, batch etc.\n",
        "train_dataset = tf.data.Dataset.zip((dx_train, dy_train)).shuffle(500).repeat().batch(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_MXh-ajzxkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# do the same operations for the validation set\n",
        "dx_valid = tf.data.Dataset.from_tensor_slices(x_test)\n",
        "dy_valid = tf.data.Dataset.from_tensor_slices(y_test).map(lambda z: tf.one_hot(z, 10))\n",
        "valid_dataset = tf.data.Dataset.zip((dx_valid, dy_valid)).shuffle(500).repeat().batch(30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvRyPnIJ3Qqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create general iterator\n",
        "iterator = tf.data.Iterator.from_structure(train_dataset.output_types,\n",
        "                                               train_dataset.output_shapes)\n",
        "next_element = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ngf46X8j3XBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make datasets that we can initialize separately, but using the same structure via the common iterator\n",
        "training_init_op = iterator.make_initializer(train_dataset)\n",
        "validation_init_op = iterator.make_initializer(valid_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ENZf6RyM3cTG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_model(in_data):\n",
        "    bn = tf.layers.batch_normalization(in_data)\n",
        "    fc1 = tf.layers.dense(bn, 50)\n",
        "    fc2 = tf.layers.dense(fc1, 50)\n",
        "    fc2 = tf.layers.dropout(fc2)\n",
        "    fc3 = tf.layers.dense(fc2, 10)\n",
        "    return fc3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IsDzAYKo3i7I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create the neural network model\n",
        "logits = nn_model(next_element[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqnnrQgZ3wNG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# add the optimizer and loss\n",
        "loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=next_element[1], logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
        "# get accuracy\n",
        "prediction = tf.argmax(logits, 1)\n",
        "equality = tf.equal(prediction, tf.argmax(next_element[1], 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(equality, tf.float32))\n",
        "init_op = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WUsQ_g4330Qg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run the training\n",
        "epochs = 600\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "    sess.run(training_init_op)\n",
        "    for i in range(epochs):\n",
        "        l, _, acc = sess.run([loss, optimizer, accuracy])\n",
        "        if i % 50 == 0:\n",
        "            print(\"Epoch: {}, loss: {:.3f}, training accuracy: {:.2f}%\".format(i, l, acc * 100))\n",
        "    # now setup the validation run\n",
        "    valid_iters = 100\n",
        "    # re-initialize the iterator, but this time with validation data\n",
        "    sess.run(validation_init_op)\n",
        "    avg_acc = 0\n",
        "    for i in range(valid_iters):\n",
        "        acc = sess.run([accuracy])\n",
        "        avg_acc += acc[0]\n",
        "    print(\"Average validation set accuracy over {} iterations is {:.2f}%\".format(valid_iters,                                                                              (avg_acc / valid_iters) * 100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjwjw11qdbi8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**5 [optativo] (1 punto extra). Implementar el modelo en TensorFlow Eager o TF de bajo nivel, sin usar tf.keras.** El bucle de entrenamiento batch a batch también tiene que hacerse utilizando operaciones de bajo nivel. En resumen, no vale utilizar capas de Keras como convoluciones o Dense ni funciones como .fit(), hay que implementarlas. Podéis programar el modelo sin embargo utilizando Keras subclassing (tf.keras.Model).\n"
      ]
    },
    {
      "metadata": {
        "id": "csMIZYQxhVfi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AW75skhgdSgu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**6 [optativo] (0.5 puntos extra). Utilizar data augmentation.** Utilizar la técnica de data augmentation y comentar los resultados. Esto puede conseguirse con la clase [ImageDataGenerator](https://keras.io/preprocessing/image/#imagedatagenerator-class) de Keras o con tf.data.\n",
        "\n",
        "\n",
        "Notas: \n",
        "* Recuerda que suele ser útil partir los datos en training/validation para tener una buena estimación de los valores que nuestro modelo tendrá en los datos de test, así como comprobar que no estamos cayendo en overfitting. Una posible partición puede ser 80 / 20.\n",
        "* No es necesario mostrar en el notebook las trazas de entrenamiento de todos los modelos entrenados, si bien una buena idea seria guardar gráficas de esos entrenamientos para su análisis. Sin embargo, recordar que **se debe mostrar el entrenamiento completo del modelo final obtenido y la evaluación de los datos de test con este modelo**.\n",
        "* Las imágenes **no están normalizadas**. Hay que normalizarlas (píxeles de 0 a 255 tienen que ir a un número real entre 0 y 1).\n",
        "* El test set del problema tiene imágenes un poco más \"fáciles\", por lo que es posible encontrarse con métricas en el test set bastante mejores que lo que vemos en el validation set de nuestro modelo.\n",
        "* No es necesario probar modelos tan grandes que tarden horas en entrenar. Es posible alcanzar la accuracy requerida con modelos que entrenan rápido en GPUs.\n",
        "* Ojo: los datos devueltos por las funciones *load_train_set* y *load_test_set* están ordenados personaje a personaje."
      ]
    },
    {
      "metadata": {
        "id": "U-h7UmO0wY8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from timeit import default_timer as timer\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "image_height = 64\n",
        "image_width = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "013X0oqHdLsQ",
        "colab_type": "code",
        "outputId": "84db1ae8-732f-4867-8729-e08b562b146d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "#ImageDataGenerator augments images, creating multiple versions of the same image\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "#Se cargan los datos descargados y almacenados en mi onedrive\n",
        "training_set = train_datagen.flow_from_directory('/root/.keras/datasets/simpsons',\n",
        "                                                 target_size = (image_height, image_width),\n",
        "                                                 batch_size = 50,\n",
        "                                                 class_mode = 'categorical')\n",
        "test_set = test_datagen.flow_from_directory('/root/.keras/datasets/simpsons_testset',\n",
        "                                            target_size = (image_height, image_width),\n",
        "                                            batch_size = 50,\n",
        "                                            class_mode = 'categorical')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18992 train samples\n",
            "890 test samples\n",
            "(18992, 64, 64, 3)\n",
            "(18992, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HqKRJFW2wE-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialising the CNN\n",
        "predator = Sequential()\n",
        "# Step 1 - Convolution\n",
        "predator.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(image_height, image_width, 3)))\n",
        "# Step 2 - Pooling\n",
        "predator.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# Adding a second convolutional layer\n",
        "predator.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "predator.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# Adding a third convolutional layer\n",
        "predator.add(Conv2D(256, (3, 3), activation=\"relu\"))\n",
        "predator.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# Adding a fourth convolutional layer\n",
        "predator.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "predator.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# Adding a fifth convolutional layer\n",
        "predator.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "predator.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# Step 3 - Flattening\n",
        "predator.add(Flatten())    #flattens the 3D image array to a single row array\n",
        "# Step 4 - Full connection\n",
        "predator.add(Dense(units=32, activation=\"relu\"))\n",
        "predator.add(Dense(units=47, activation=\"softmax\"))   #output layer with 10 neurons.. each corresponding to a character\n",
        "# Compiling the CNN\n",
        "predator.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fFAZgfv_fSkl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predator.fit_generator(training_set,\n",
        "                         steps_per_epoch = 16,\n",
        "                         epochs = 100,\n",
        "                         validation_data = test_set,\n",
        "                         validation_steps = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qTXcSwWIeFM_",
        "colab_type": "code",
        "outputId": "d93c8985-5486-4779-9617-710fbfcca316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_85 (Dense)             (None, 4096)              50335744  \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 47)                192559    \n",
            "=================================================================\n",
            "Total params: 67,309,615\n",
            "Trainable params: 67,309,615\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d_ioim0TeOjR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jH7PQrCeVbH",
        "colab_type": "code",
        "outputId": "47e9a950-c4fb-4c10-d5c2-79bf1aca27bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                   validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18992 samples, validate on 890 samples\n",
            "Epoch 1/20\n",
            "18992/18992 [==============================] - 11s 573us/step - loss: 2.8367 - acc: 0.1025 - val_loss: 2.9846 - val_acc: 0.0697\n",
            "Epoch 2/20\n",
            "18992/18992 [==============================] - 8s 409us/step - loss: 2.7647 - acc: 0.1277 - val_loss: 2.8389 - val_acc: 0.1348\n",
            "Epoch 3/20\n",
            "18992/18992 [==============================] - 8s 408us/step - loss: 2.6733 - acc: 0.1647 - val_loss: 2.7583 - val_acc: 0.1236\n",
            "Epoch 4/20\n",
            "18992/18992 [==============================] - 8s 409us/step - loss: 2.5475 - acc: 0.2025 - val_loss: 2.6389 - val_acc: 0.1978\n",
            "Epoch 5/20\n",
            "18992/18992 [==============================] - 8s 412us/step - loss: 2.4227 - acc: 0.2445 - val_loss: 2.5212 - val_acc: 0.2270\n",
            "Epoch 6/20\n",
            "18992/18992 [==============================] - 8s 413us/step - loss: 2.3215 - acc: 0.2836 - val_loss: 2.3082 - val_acc: 0.3090\n",
            "Epoch 7/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 2.1868 - acc: 0.3214 - val_loss: 2.2481 - val_acc: 0.3281\n",
            "Epoch 8/20\n",
            "18992/18992 [==============================] - 8s 410us/step - loss: 2.0910 - acc: 0.3553 - val_loss: 2.0791 - val_acc: 0.3730\n",
            "Epoch 9/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 1.9834 - acc: 0.3895 - val_loss: 1.9899 - val_acc: 0.4393\n",
            "Epoch 10/20\n",
            "18992/18992 [==============================] - 8s 413us/step - loss: 1.8935 - acc: 0.4200 - val_loss: 1.7535 - val_acc: 0.4798\n",
            "Epoch 11/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 1.8090 - acc: 0.4454 - val_loss: 1.6444 - val_acc: 0.5079\n",
            "Epoch 12/20\n",
            "18992/18992 [==============================] - 8s 410us/step - loss: 1.7147 - acc: 0.4773 - val_loss: 1.8528 - val_acc: 0.4371\n",
            "Epoch 13/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 1.6398 - acc: 0.4942 - val_loss: 1.5347 - val_acc: 0.5483\n",
            "Epoch 14/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 1.5760 - acc: 0.5169 - val_loss: 1.4375 - val_acc: 0.5596\n",
            "Epoch 15/20\n",
            "18992/18992 [==============================] - 8s 410us/step - loss: 1.5055 - acc: 0.5412 - val_loss: 1.3703 - val_acc: 0.5944\n",
            "Epoch 16/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 1.4404 - acc: 0.5598 - val_loss: 1.2830 - val_acc: 0.5955\n",
            "Epoch 17/20\n",
            "18992/18992 [==============================] - 8s 410us/step - loss: 1.3707 - acc: 0.5848 - val_loss: 1.2099 - val_acc: 0.6202\n",
            "Epoch 18/20\n",
            "18992/18992 [==============================] - 8s 412us/step - loss: 1.3099 - acc: 0.6007 - val_loss: 1.2895 - val_acc: 0.6067\n",
            "Epoch 19/20\n",
            "18992/18992 [==============================] - 8s 412us/step - loss: 1.2670 - acc: 0.6129 - val_loss: 1.3525 - val_acc: 0.5719\n",
            "Epoch 20/20\n",
            "18992/18992 [==============================] - 8s 411us/step - loss: 1.1944 - acc: 0.6324 - val_loss: 0.9846 - val_acc: 0.6955\n",
            "Test loss: 0.9846352172701547\n",
            "Test accuracy: 0.695505617709642\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}